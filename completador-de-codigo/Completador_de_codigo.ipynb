{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ToTGJ45vqkqx"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"SELECT * FROM customers WHERE active = 1;\",\n",
        "    \"INSERT INTO products (name, price) VALUES ('Laptop', 1200);\",\n",
        "    \"UPDATE employees SET salary = 5000 WHERE department = 'HR';\",\n",
        "    \"DELETE FROM orders WHERE order_date < '2023-01-01';\",\n",
        "    \"CREATE TABLE users (id INT PRIMARY KEY, username VARCHAR(50));\",\n",
        "    \"ALTER TABLE invoices ADD COLUMN total DECIMAL(10, 2);\",\n",
        "    \"DROP TABLE IF EXISTS temp_data;\",\n",
        "    \"SELECT name, COUNT(*) AS total FROM sales GROUP BY name;\",\n",
        "    \"JOIN addresses ON customers.id = addresses.customer_id;\",\n",
        "    \"ORDER BY created_at DESC LIMIT 10;\",\n",
        "    \"WHERE status = 'pending' AND priority > 5;\",\n",
        "    \"HAVING SUM(amount) > 1000;\",\n",
        "    \"GRANT SELECT, INSERT ON database.* TO 'user'@'localhost';\",\n",
        "    \"REVOKE ALL PRIVILEGES ON employees FROM 'intern'@'%';\",\n",
        "    \"BEGIN TRANSACTION; COMMIT;\",\n",
        "    \"ROLLBACK TO SAVEPOINT before_update;\",\n",
        "    \"EXPLAIN SELECT * FROM logs WHERE type = 'error';\",\n",
        "    \"INDEX idx_name ON employees (last_name);\",\n",
        "    \"PRIMARY KEY (order_id, product_id);\",\n",
        "    \"FOREIGN KEY (customer_id) REFERENCES customers(id);\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB0oLBfrr3zJ"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Qx3wF5irs7T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenização e vocabulário\n",
        "tokens = [word.lower() for sentence in corpus for word in sentence.split()]\n",
        "vocab = Counter(tokens)\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Mapeamento palavra para índice\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnkliBk8vPDR"
      },
      "source": [
        "Nesta etapa, vamos criar o modelo de linguagem que será treinado para prever a próxima palavra com base no contexto das palavras anteriores. O modelo será uma rede neural baseada em embeddings e LSTMs, que são adequados para lidar com sequências de texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Estrutura do Modelo**\n",
        "\n",
        "O modelo será composto por:\n",
        "\n",
        "1. **Camada de Embedding**  \n",
        "   - Converte os índices das palavras em vetores densos de tamanho fixo (representação vetorial das palavras).  \n",
        "   - Tamanho do embedding (`embedding_dim`) é um hiperparâmetro.\n",
        "\n",
        "2. **Camada LSTM**  \n",
        "   - Processa as sequências de embeddings e mantém informações sobre o contexto anterior.  \n",
        "   - Tamanho do estado oculto (`hidden_dim`) é outro hiperparâmetro.\n",
        "\n",
        "3. **Camada Linear (Fully Connected)**  \n",
        "   - Projeta a saída da LSTM para o espaço do vocabulário, gerando scores para cada palavra no vocabulário.\n",
        "\n",
        "4. **Softmax (aplicado durante inferência)**  \n",
        "   - Converte os scores para probabilidades de cada palavra no vocabulário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CWa61z4cr0p8"
      },
      "outputs": [],
      "source": [
        "class SQLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jtsa-f9wJ3q"
      },
      "source": [
        "Na sequência, vamos criar um **Dataset** e um **DataLoader** para alimentar o modelo com dados durante o treinamento. O Dataset será responsável por preparar as sequências de entrada e saída a partir do corpus, enquanto o DataLoader organizará os dados em lotes (`batches`) para treinamento eficiente.\n",
        "\n",
        "---\n",
        "\n",
        "### **Estrutura do Dataset**\n",
        "\n",
        "O Dataset utilizará o corpus pré-processado e fará o seguinte:\n",
        "\n",
        "1. **Divisão em Sequências**  \n",
        "   - Cada sentença será dividida em sequências de comprimento fixo (`seq_length`).  \n",
        "   - Por exemplo, dado o texto: `\"SELECT * FROM customers\"`, com `seq_length=3`:\n",
        "     - Entrada (`x`): `\"SELECT * FROM\"`\n",
        "     - Saída (`y`): `\"* FROM customers\"`\n",
        "\n",
        "2. **Mapeamento para Índices**  \n",
        "   - Cada palavra será convertida para seu índice correspondente no vocabulário (`word_to_idx`).\n",
        "\n",
        "3. **Par Entrada-Saída**  \n",
        "   - Para cada sequência de entrada (`x`), haverá uma sequência de saída (`y`), que corresponde às próximas palavras.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VE6868EqsF81"
      },
      "outputs": [],
      "source": [
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, corpus, seq_length=3):\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "\n",
        "        for sentence in corpus:\n",
        "            tokens = sentence.lower().split()  # Dividir a frase em palavras\n",
        "            indices = [word_to_idx[word] for word in tokens]  # Mapear palavras para índices\n",
        "            for i in range(len(indices) - self.seq_length):\n",
        "                self.data.append((\n",
        "                    torch.tensor(indices[i:i+self.seq_length]),\n",
        "                    torch.tensor(indices[i+1:i+1+self.seq_length])\n",
        "                ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = SQLDataset(corpus, seq_length=3)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz5aYuyZxb-4"
      },
      "source": [
        "Agora que temos o **modelo**, o **Dataset** e o **DataLoader**, o próximo passo é implementar o loop de treinamento. O objetivo do treinamento é ajustar os pesos do modelo para minimizar a perda (loss), que mede a diferença entre as previsões do modelo e as saídas reais.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapas do Treinamento**\n",
        "\n",
        "1. **Definir Função de Perda e Otimizador**  \n",
        "   - A função de perda será a `CrossEntropyLoss`, que é adequada para problemas de classificação, como prever a próxima palavra.  \n",
        "   - O otimizador será o `Adam`, que é eficiente para redes neurais profundas.\n",
        "\n",
        "2. **Loop de Treinamento**  \n",
        "   - Para cada época:\n",
        "     - Iterar sobre os lotes do DataLoader.\n",
        "     - Passar o lote pelo modelo para obter as previsões.\n",
        "     - Calcular a perda comparando as previsões com as saídas reais.\n",
        "     - Atualizar os pesos do modelo usando o otimizador.\n",
        "\n",
        "3. **Monitorar o Progresso**  \n",
        "   - Exibir a perda a cada época para acompanhar o aprendizado do modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR6Tw1GfsaCC",
        "outputId": "15be01b1-f035-405b-ec85-94109a06b874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4.5007\n",
            "Epoch 2, Loss: 4.0063\n",
            "Epoch 3, Loss: 3.4762\n",
            "Epoch 4, Loss: 2.4731\n",
            "Epoch 5, Loss: 1.1435\n",
            "Epoch 6, Loss: 1.0793\n",
            "Epoch 7, Loss: 0.7201\n",
            "Epoch 8, Loss: 0.8749\n",
            "Epoch 9, Loss: 0.6145\n",
            "Epoch 10, Loss: 0.4538\n",
            "Epoch 11, Loss: 0.3816\n",
            "Epoch 12, Loss: 0.7913\n",
            "Epoch 13, Loss: 0.1348\n",
            "Epoch 14, Loss: 0.1684\n",
            "Epoch 15, Loss: 0.1087\n",
            "Epoch 16, Loss: 0.1073\n",
            "Epoch 17, Loss: 0.1309\n",
            "Epoch 18, Loss: 0.5731\n",
            "Epoch 19, Loss: 0.1174\n",
            "Epoch 20, Loss: 0.0730\n",
            "Epoch 21, Loss: 0.7512\n",
            "Epoch 22, Loss: 0.0378\n",
            "Epoch 23, Loss: 0.0483\n",
            "Epoch 24, Loss: 0.0260\n",
            "Epoch 25, Loss: 0.0603\n",
            "Epoch 26, Loss: 0.0285\n",
            "Epoch 27, Loss: 0.0354\n",
            "Epoch 28, Loss: 0.0248\n",
            "Epoch 29, Loss: 0.0161\n",
            "Epoch 30, Loss: 0.5270\n",
            "Epoch 31, Loss: 0.0322\n",
            "Epoch 32, Loss: 0.0152\n",
            "Epoch 33, Loss: 0.5958\n",
            "Epoch 34, Loss: 0.7004\n",
            "Epoch 35, Loss: 0.0132\n",
            "Epoch 36, Loss: 0.3817\n",
            "Epoch 37, Loss: 0.0065\n",
            "Epoch 38, Loss: 0.0267\n",
            "Epoch 39, Loss: 0.0159\n",
            "Epoch 40, Loss: 0.0100\n",
            "Epoch 41, Loss: 0.0073\n",
            "Epoch 42, Loss: 0.0049\n",
            "Epoch 43, Loss: 0.5871\n",
            "Epoch 44, Loss: 0.5897\n",
            "Epoch 45, Loss: 0.0119\n",
            "Epoch 46, Loss: 0.4889\n",
            "Epoch 47, Loss: 0.7459\n",
            "Epoch 48, Loss: 0.4401\n",
            "Epoch 49, Loss: 0.0223\n",
            "Epoch 50, Loss: 0.0040\n",
            "Epoch 51, Loss: 0.0086\n",
            "Epoch 52, Loss: 0.4419\n",
            "Epoch 53, Loss: 0.0105\n",
            "Epoch 54, Loss: 0.0117\n",
            "Epoch 55, Loss: 0.0073\n",
            "Epoch 56, Loss: 0.3516\n",
            "Epoch 57, Loss: 0.0113\n",
            "Epoch 58, Loss: 0.0081\n",
            "Epoch 59, Loss: 0.4477\n",
            "Epoch 60, Loss: 0.0077\n",
            "Epoch 61, Loss: 0.5306\n",
            "Epoch 62, Loss: 0.3909\n",
            "Epoch 63, Loss: 0.0094\n",
            "Epoch 64, Loss: 0.0053\n",
            "Epoch 65, Loss: 0.0087\n",
            "Epoch 66, Loss: 0.0082\n",
            "Epoch 67, Loss: 0.5315\n",
            "Epoch 68, Loss: 0.0029\n",
            "Epoch 69, Loss: 0.6589\n",
            "Epoch 70, Loss: 0.0043\n",
            "Epoch 71, Loss: 0.0089\n",
            "Epoch 72, Loss: 0.0049\n",
            "Epoch 73, Loss: 0.0043\n",
            "Epoch 74, Loss: 0.0054\n",
            "Epoch 75, Loss: 0.0029\n",
            "Epoch 76, Loss: 0.0028\n",
            "Epoch 77, Loss: 0.6429\n",
            "Epoch 78, Loss: 0.0034\n",
            "Epoch 79, Loss: 0.0032\n",
            "Epoch 80, Loss: 0.3510\n",
            "Epoch 81, Loss: 0.4776\n",
            "Epoch 82, Loss: 0.5969\n",
            "Epoch 83, Loss: 0.2772\n",
            "Epoch 84, Loss: 0.6025\n",
            "Epoch 85, Loss: 0.0046\n",
            "Epoch 86, Loss: 0.6928\n",
            "Epoch 87, Loss: 0.0018\n",
            "Epoch 88, Loss: 0.0019\n",
            "Epoch 89, Loss: 0.7094\n",
            "Epoch 90, Loss: 0.0013\n",
            "Epoch 91, Loss: 0.0020\n",
            "Epoch 92, Loss: 0.6339\n",
            "Epoch 93, Loss: 0.4862\n",
            "Epoch 94, Loss: 0.4931\n",
            "Epoch 95, Loss: 0.0030\n",
            "Epoch 96, Loss: 0.0069\n",
            "Epoch 97, Loss: 0.0032\n",
            "Epoch 98, Loss: 0.0059\n",
            "Epoch 99, Loss: 0.0036\n",
            "Epoch 100, Loss: 0.0015\n",
            "Epoch 101, Loss: 0.0015\n",
            "Epoch 102, Loss: 0.0036\n",
            "Epoch 103, Loss: 0.4091\n",
            "Epoch 104, Loss: 0.0013\n",
            "Epoch 105, Loss: 0.0037\n",
            "Epoch 106, Loss: 0.0028\n",
            "Epoch 107, Loss: 0.0024\n",
            "Epoch 108, Loss: 0.0033\n",
            "Epoch 109, Loss: 0.4902\n",
            "Epoch 110, Loss: 0.3448\n",
            "Epoch 111, Loss: 0.4682\n",
            "Epoch 112, Loss: 0.2805\n",
            "Epoch 113, Loss: 0.0015\n",
            "Epoch 114, Loss: 0.5282\n",
            "Epoch 115, Loss: 0.0013\n",
            "Epoch 116, Loss: 0.0015\n",
            "Epoch 117, Loss: 0.5201\n",
            "Epoch 118, Loss: 0.0010\n",
            "Epoch 119, Loss: 0.6921\n",
            "Epoch 120, Loss: 0.0009\n",
            "Epoch 121, Loss: 0.0009\n",
            "Epoch 122, Loss: 0.0010\n",
            "Epoch 123, Loss: 0.0011\n",
            "Epoch 124, Loss: 0.0008\n",
            "Epoch 125, Loss: 0.4959\n",
            "Epoch 126, Loss: 0.0016\n",
            "Epoch 127, Loss: 0.4459\n",
            "Epoch 128, Loss: 0.3056\n",
            "Epoch 129, Loss: 0.0006\n",
            "Epoch 130, Loss: 0.0010\n",
            "Epoch 131, Loss: 0.5309\n",
            "Epoch 132, Loss: 0.0020\n",
            "Epoch 133, Loss: 0.0011\n",
            "Epoch 134, Loss: 0.6910\n",
            "Epoch 135, Loss: 0.3142\n",
            "Epoch 136, Loss: 0.4370\n",
            "Epoch 137, Loss: 0.4256\n",
            "Epoch 138, Loss: 0.6452\n",
            "Epoch 139, Loss: 0.0012\n",
            "Epoch 140, Loss: 0.3047\n",
            "Epoch 141, Loss: 0.0018\n",
            "Epoch 142, Loss: 0.3341\n",
            "Epoch 143, Loss: 0.0011\n",
            "Epoch 144, Loss: 0.5874\n",
            "Epoch 145, Loss: 0.0006\n",
            "Epoch 146, Loss: 0.0011\n",
            "Epoch 147, Loss: 0.0005\n",
            "Epoch 148, Loss: 0.2937\n",
            "Epoch 149, Loss: 0.0016\n",
            "Epoch 150, Loss: 0.5927\n",
            "Epoch 151, Loss: 0.0010\n",
            "Epoch 152, Loss: 0.0012\n",
            "Epoch 153, Loss: 0.6272\n",
            "Epoch 154, Loss: 0.2710\n",
            "Epoch 155, Loss: 0.3860\n",
            "Epoch 156, Loss: 0.0007\n",
            "Epoch 157, Loss: 0.0013\n",
            "Epoch 158, Loss: 0.0015\n",
            "Epoch 159, Loss: 0.0007\n",
            "Epoch 160, Loss: 0.0005\n",
            "Epoch 161, Loss: 0.0014\n",
            "Epoch 162, Loss: 0.0004\n",
            "Epoch 163, Loss: 0.5834\n",
            "Epoch 164, Loss: 0.6117\n",
            "Epoch 165, Loss: 0.0012\n",
            "Epoch 166, Loss: 0.4351\n",
            "Epoch 167, Loss: 0.0036\n",
            "Epoch 168, Loss: 0.5444\n",
            "Epoch 169, Loss: 0.4806\n",
            "Epoch 170, Loss: 0.0003\n",
            "Epoch 171, Loss: 0.5660\n",
            "Epoch 172, Loss: 0.0002\n",
            "Epoch 173, Loss: 0.0002\n",
            "Epoch 174, Loss: 0.0002\n",
            "Epoch 175, Loss: 0.0011\n",
            "Epoch 176, Loss: 0.3774\n",
            "Epoch 177, Loss: 0.5061\n",
            "Epoch 178, Loss: 0.2768\n",
            "Epoch 179, Loss: 0.0015\n",
            "Epoch 180, Loss: 0.0011\n",
            "Epoch 181, Loss: 0.0010\n",
            "Epoch 182, Loss: 0.0002\n",
            "Epoch 183, Loss: 0.0011\n",
            "Epoch 184, Loss: 0.4228\n",
            "Epoch 185, Loss: 0.0010\n",
            "Epoch 186, Loss: 0.0011\n",
            "Epoch 187, Loss: 0.0007\n",
            "Epoch 188, Loss: 0.0010\n",
            "Epoch 189, Loss: 0.0003\n",
            "Epoch 190, Loss: 0.6618\n",
            "Epoch 191, Loss: 0.7472\n",
            "Epoch 192, Loss: 0.0007\n",
            "Epoch 193, Loss: 0.4908\n",
            "Epoch 194, Loss: 0.0002\n",
            "Epoch 195, Loss: 0.0002\n",
            "Epoch 196, Loss: 0.0003\n",
            "Epoch 197, Loss: 0.0009\n",
            "Epoch 198, Loss: 0.2768\n",
            "Epoch 199, Loss: 0.4502\n",
            "Epoch 200, Loss: 0.2879\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SQLLM(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZYzfLKsuc3"
      },
      "source": [
        "# Função de completar texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S1c4dxHAssc-"
      },
      "outputs": [],
      "source": [
        "def complete_text(seed_text, num_words=5, temperature=0.7):\n",
        "    model.eval()\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            inputs = torch.tensor(\n",
        "                [word_to_idx[word] for word in words[-3:]]  # Usa contexto de 3 palavras\n",
        "            ).unsqueeze(0).to(device)\n",
        "\n",
        "            output, _ = model(inputs)\n",
        "            probabilities = torch.softmax(output[0, -1] / temperature, dim=0)\n",
        "            next_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            words.append(idx_to_word[next_idx])\n",
        "\n",
        "    return \" \".join(words).capitalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaV5y_3as5Ug",
        "outputId": "fcf34cbc-fbca-49f2-bff3-0abfe5ec1f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select * from logs where type = 'error';\n",
            "Add column total decimal(10, 2); group\n",
            "Rollback to savepoint before_update; where status =\n"
          ]
        }
      ],
      "source": [
        "# Exemplos de prompts\n",
        "print(complete_text(\"SELECT * FROM\", num_words=5))\n",
        "print(complete_text(\"ADD COLUMN total\", num_words=3))\n",
        "print(complete_text(\"ROLLBACK TO SAVEPOINT\", num_words=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jS3ajy7Gs7CL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
